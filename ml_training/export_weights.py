#!/usr/bin/env python3
"""Export trained logreg and MLP weights to a C header for on-device inference.

Usage:
    python ml_training/export_weights.py --train-dir ml_out/train --out src/mlWeights.h

Trains both models on the full dataset (no split) and exports:
  - StandardScaler mean/std
  - LogisticRegression coef/intercept
  - MLP weights/biases for (64, 32) architecture
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler


def load_dataset(train_dir: Path):
    meta = json.loads((train_dir / "meta.json").read_text())
    window_samples = int(meta["window_samples"])
    channels = meta["channels"]
    if channels != ["ax", "ay", "az", "gx", "gy", "gz"]:
        raise SystemExit(f"Unexpected channels: {channels}")
    X = np.fromfile(train_dir / "windows_i16le.bin", dtype="<i2").reshape(-1, window_samples, 6).astype(np.float32)
    y = np.fromfile(train_dir / "labels_u8.bin", dtype=np.uint8)
    if X.shape[0] != y.shape[0]:
        raise SystemExit(f"Mismatched rows: X={X.shape[0]} y={y.shape[0]}")
    return meta, X, y


def featurize_rich(X: np.ndarray) -> np.ndarray:
    mean = X.mean(axis=1)
    std = X.std(axis=1)
    mx = np.abs(X).max(axis=1)
    absmean = np.abs(X).mean(axis=1)
    a = X[:, :, 0:3]
    g = X[:, :, 3:6]
    amag = np.sqrt((a * a).sum(axis=2))
    gmag = np.sqrt((g * g).sum(axis=2))
    return np.concatenate([
        mean, std, mx, absmean,
        amag.mean(axis=1, keepdims=True),
        amag.std(axis=1, keepdims=True),
        amag.max(axis=1, keepdims=True),
        gmag.mean(axis=1, keepdims=True),
        gmag.std(axis=1, keepdims=True),
        gmag.max(axis=1, keepdims=True),
    ], axis=1)


def featurize_summary(X: np.ndarray) -> np.ndarray:
    mean = X.mean(axis=1)
    std = X.std(axis=1)
    mx = np.abs(X).max(axis=1)
    return np.concatenate([mean, std, mx], axis=1)


def fmt_array(name: str, arr: np.ndarray, cols: int = 6) -> str:
    flat = arr.flatten()
    lines = [f"static const float {name}[{len(flat)}] = {{"]
    for i in range(0, len(flat), cols):
        chunk = flat[i:i + cols]
        lines.append("\t" + ", ".join(f"{v:.8f}f" for v in chunk) + ",")
    lines.append("};")
    return "\n".join(lines)


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--train-dir", type=Path, default=Path("ml_out/train"))
    ap.add_argument("--out", type=Path, default=Path("src/mlWeights.h"))
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    meta, X, y = load_dataset(args.train_dir)
    window_samples = int(meta["window_samples"])

    # --- Logistic Regression (summary features, 18-dim) ---
    F_sum = featurize_summary(X)
    scaler_lr = StandardScaler().fit(F_sum)
    F_sum_s = scaler_lr.transform(F_sum)
    lr = LogisticRegression(max_iter=5000, random_state=args.seed)
    lr.fit(F_sum_s, y)
    print(f"LogReg train accuracy: {lr.score(F_sum_s, y):.3f}")

    # --- MLP (rich features, 30-dim, hidden 64,32) ---
    F_rich = featurize_rich(X)
    scaler_mlp = StandardScaler().fit(F_rich)
    F_rich_s = scaler_mlp.transform(F_rich)
    mlp = MLPClassifier(
        hidden_layer_sizes=(64, 32),
        activation="relu",
        solver="adam",
        alpha=1e-4,
        max_iter=800,
        random_state=args.seed,
        early_stopping=len(y) >= 80,
        n_iter_no_change=10,
    )
    mlp.fit(F_rich_s, y)
    print(f"MLP train accuracy: {mlp.score(F_rich_s, y):.3f}")

    # --- Generate C header ---
    lines = [
        "#pragma once",
        "// Auto-generated by ml_training/export_weights.py",
        "// Do not edit manually.",
        "",
        f"#define ML_WINDOW_SAMPLES {window_samples}",
        f"#define ML_CHANNELS 6",
        f"#define ML_LR_FEATURES 18   // summary: mean(6) + std(6) + absmax(6)",
        f"#define ML_MLP_FEATURES 30  // rich: mean(6) + std(6) + absmax(6) + absmean(6) + mag_stats(6)",
        f"#define ML_MLP_H1 64",
        f"#define ML_MLP_H2 32",
        "",
        "// === Logistic Regression (summary features) ===",
        "",
        fmt_array("mlLrScalerMean", scaler_lr.mean_),
        "",
        fmt_array("mlLrScalerScale", scaler_lr.scale_),
        "",
        fmt_array("mlLrCoef", lr.coef_.flatten()),
        "",
        f"static const float mlLrIntercept = {lr.intercept_[0]:.8f}f;",
        "",
        "// === MLP (rich features, 64->32->1) ===",
        "",
        fmt_array("mlMlpScalerMean", scaler_mlp.mean_),
        "",
        fmt_array("mlMlpScalerScale", scaler_mlp.scale_),
        "",
        "// Layer 1: 30 -> 64 (weights stored row-major: W[out][in])",
        fmt_array("mlMlpW1", mlp.coefs_[0].T),  # sklearn stores (in, out), we want (out, in)
        "",
        fmt_array("mlMlpB1", mlp.intercepts_[0]),
        "",
        "// Layer 2: 64 -> 32",
        fmt_array("mlMlpW2", mlp.coefs_[1].T),
        "",
        fmt_array("mlMlpB2", mlp.intercepts_[1]),
        "",
        "// Output layer: 32 -> 1",
        fmt_array("mlMlpW3", mlp.coefs_[2].T),
        "",
        f"static const float mlMlpB3 = {mlp.intercepts_[2][0]:.8f}f;",
        "",
    ]

    args.out.parent.mkdir(parents=True, exist_ok=True)
    args.out.write_text("\n".join(lines) + "\n")
    print(f"Wrote {args.out} ({args.out.stat().st_size} bytes)")


if __name__ == "__main__":
    main()
