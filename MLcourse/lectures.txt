Page
Page number
40
of 40
(function of the parameters w, b)
ğ‘“ğ‘¤,ğ‘(ğ‘¥)
(function of x (when w and b fixed))
ğ½(ğ‘¤, ğ‘)
b
w
(function of the parameters w, b)
ğ‘“ğ‘¤,ğ‘(ğ‘¥)
(function of x (when w and b fixed))
ğ½(ğ‘¤, ğ‘)
b
w
(function of the parameters w, b)
ğ‘“ğ‘¤,ğ‘(ğ‘¥)
(function of x (when w and b fixed))
ğ½(ğ‘¤, ğ‘)
b
w
(function of the parameters w, b)
ğ‘“ğ‘¤,ğ‘(ğ‘¥)
(function of x (when w and b fixed))
ğ½(ğ‘¤, ğ‘)
b
w
(function of the parameters w, b)
ğ‘“ğ‘¤,ğ‘(ğ‘¥)
(function of x (when w and b fixed))
ğ½(ğ‘¤, ğ‘)
b
w
â€œBatchâ€ Gradient Descent
â€œBatchâ€: Each step of gradient descent
uses all the training examples.

Page
Page number
1
of 21
Linear Regression with
multiple variables
Multiple features
Andrew Ng
edited Bart Vanrumste
Machine Learning
Size (feet2) Price ($1000)
2104 460
1416 232
1534 315
852 178
â€¦ â€¦
Multiple features (variables)
Size (feet2) Number of
bedrooms
Number of
floors
Age of home
(years)
Price ($1000)
2104 5 1 45 460
1416 3 2 40 232
1534 3 2 30 315
852 2 1 36 178
â€¦ â€¦ â€¦ â€¦ â€¦
Multiple features (variables).
Notation:
= number of features
= input (features) of training example.
= value of feature in training example.
Previously (n=1):
ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥(ğ‘–) = ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘¤3ğ‘¥3 + ğ‘¤4ğ‘¥4 + ğ‘
ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥(ğ‘–) = ğ‘¤. Ô¦ğ‘¥ + ğ‘
Multi feature (n>1):
Linear Regression with
multiple variables
Vectorization
Parameters and features
Without vectorization
Without vectorization
With vectorization

Page
Page number
1
of 15
Logistic
Regression
Classification
Machine Learning
Classification
Email: Spam / Not Spam?
Online Transactions: Fraudulent (Yes / No)?
Tumor: Malignant / Benign ?
0: â€œNegative Classâ€ (e.g., benign tumor)
1: â€œPositive Classâ€ (e.g., malignant tumor)
Tumor Size
Threshold classifier output ğ‘“ğ‘¤,ğ‘(ğ‘¥) at 0.5:
If , predict â€œy = 1â€
If , predict â€œy = 0â€
Tumor Size
Malignant ?
(Yes) 1
(No) 0
ğ‘“ğ‘¤,ğ‘ ğ‘¥ â‰¥ 0,5
ğ‘“ğ‘¤,ğ‘ ğ‘¥ < 0,5
Classification: y = 0 or 1
can be > 1 or < 0
Logistic Regression:
ğ‘“ğ‘¤,ğ‘ ğ‘¥
0 â‰¤ ğ‘“ğ‘¤,ğ‘ ğ‘¥ â‰¤ 1
Logistic
Regression
Sigmoid function
Logistic function
Logistic Regression Model
Want
1
0.5
0
0 â‰¤ ğ‘“ğ‘¤,ğ‘ ğ‘¥ â‰¤ 1
ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = ğ‘”(ğ‘¤. Ô¦ğ‘¥ + ğ‘)

Page
Page number
1
of 26
Logistic
Regression
Cost function
Training set
ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = ğ‘”(ğ‘¤. Ô¦ğ‘¥ + ğ‘)
How to choose parameters
Cost function
Linear regression:
Logistic regression â€œnon-convexâ€Linear regression: â€œconvexâ€
ğ½ ğ‘¤, ğ‘ = 1
ğ‘š à·
ğ‘–=1
ğ‘š 1
2 (ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2
ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = ğ‘¤. Ô¦ğ‘¥ + ğ‘ ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = 1
1 + ğ‘’âˆ’(ğ‘¤. Ô¦ğ‘¥+ğ‘)
Logistic regression cost function
If y = 1
10
ğ¿ ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– , ğ‘¦ ğ‘– = á‰ âˆ’lğ‘›(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– ) ğ‘¦(ğ‘–) = 1
âˆ’lğ‘› (1 âˆ’ ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– ) ğ‘¦(ğ‘–) = 0
Logistic regression cost function
If y = 0
10
ğ¿ ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– , ğ‘¦ ğ‘– = á‰ âˆ’lğ‘›(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– ) ğ‘¦(ğ‘–) = 1
âˆ’lğ‘› (1 âˆ’ ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– ) ğ‘¦(ğ‘–) = 0
Loss is lowest when ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘–
predicts close to true label ğ‘¦(ğ‘–)
Logistic
Regression
Simplified cost function
and gradient descent
Page
Page number
1
of 18
Neural Networks:
Representation
Non-linear
decision
boundaryMachine Learning
Non-linear Classification
x1
x2
Neural Networks:
Representation
Model
representation I
Neural Network Representation
ğ‘¥1
ğ‘¥2
ğ‘¥3
à·œğ‘¦
ğ‘§ = ğ‘¤. Ô¦ğ‘¥ + ğ‘
ğ‘
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘”(ğ‘§) ğ‘ = à·œğ‘¦
ğ‘§
ğ‘¤. Ô¦ğ‘¥ + ğ‘
ğ‘ = ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = 1
1 + ğ‘’âˆ’(ğ‘¤. Ô¦ğ‘¥+ğ‘)
ğ‘
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘ = à·œğ‘¦
ğ‘§
Neural Network Representation
ğ‘¥1
ğ‘¥2
ğ‘¥3
à·œğ‘¦
ğ‘¥1
ğ‘¥2
ğ‘¥3
à·œğ‘¦
ğ‘¤. Ô¦ğ‘¥ + ğ‘
ğ‘§ = ğ‘¤. Ô¦ğ‘¥ + ğ‘
ğ‘ = ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ = 1
1 + ğ‘’âˆ’(ğ‘¤. Ô¦ğ‘¥+ğ‘)
ğ‘”(ğ‘§)
Neural Network Representation (dot-
product notation)
ğ‘¥1
ğ‘¥2
ğ‘¥3
à·œğ‘¦
ğ‘1
1
ğ‘2
1
ğ‘3
1
ğ‘4
1
ğ‘§1
1 = ğ‘¤1
1 âˆ™ Ô¦ğ‘¥ + ğ‘1
[1], ğ‘ 1
[1] = ğœ(ğ‘§1
1 )
ğ‘§2
1 = ğ‘¤2
1 âˆ™ Ô¦ğ‘¥ + ğ‘2
[1], ğ‘ 2
[1] = ğœ(ğ‘§2
1 )
ğ‘§3
1 = ğ‘¤3
1 âˆ™ Ô¦ğ‘¥ + ğ‘3
[1], ğ‘ 3
[1] = ğœ(ğ‘§3
1 )
ğ‘§4
1 = ğ‘¤4
1 âˆ™ Ô¦ğ‘¥ + ğ‘4
[1], ğ‘ 4
[1] = ğœ(ğ‘§4
1 )
Page
Page number
1
of 27
Neural Networks:
Implementation
Cost function:
Cross Entropy
LossMachine Learning
Recognizing cats, dogs, and baby chicks
X à·œğ‘¦
2 3 24 41 13
2
Andrew Ng
Neural Network (Classification)
Binary classification
1 output unit
Multi-class classification (K classes)
K output units
total no. of layers in network
no. of units in layer l
Other Cat Dog Chick
E.g. , , ,
Andrew Ng
Cost Function
Andrew Ng
Softmax
Neural Networks:
Implementation
5
Andrew Ng
Softmax layer
X à·œğ‘¦
6
Page
Page number
1
of 24
Advice for applying
machine learning
Deciding what
to try next
Debugging a learning algorithm:
You have implemented regularized linear regression to predict housing prices.
However, when you test your hypothesis on a new set of houses, you find that it
makes unacceptably large errors in its predictions. What should you try next?
- Get more training examples
- Try smaller sets of features
- Try getting additional features
- Try adding polynomial features
- Try decreasing
- Try increasing
ğ½ ğ‘¤, ğ‘ = 1
2ğ‘š à·
ğ‘–=1
ğ‘š
(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2 + Î»
2ğ‘š à·
ğ‘—=1
ğ‘›
ğ‘¤ğ‘—
2
Machine learning diagnostic:
Diagnostic: A test that you can run to gain insight what
is/isnâ€™t working with a learning algorithm, and gain
guidance as to how best to improve its performance.
Diagnostics can take time to implement, but doing so
can be a very good use of your time.
Advice for applying
machine learning
Model selection and
training/validation/test
sets
Evaluating your hypothesis
Dataset:
Size Price
2104 400
1600 330
2400 369
1416 232
3000 540
1985 300
1534 315
1427 199
1380 212
1494 243
Train/validation/test error
Training error:
Cross Validation error:
Test error:
ğ½ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› = 1
2ğ‘š à·
ğ‘–=1
ğ‘š
(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2
ğ½ğ¶ğ‘‰ = 1
2ğ‘šğ¶ğ‘‰
à·
ğ‘–=1
ğ‘šğ¶ğ‘‰
(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2
ğ½ğ‘¡ğ‘’ğ‘ ğ‘¡ = 1
2ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡
à·
ğ‘–=1
ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡
(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2
ğ½ ğ‘¤, ğ‘ = 1
2ğ‘š à·
ğ‘–=1
ğ‘š
(ğ‘“ğ‘¤,ğ‘ Ô¦ğ‘¥ ğ‘– âˆ’ ğ‘¦ ğ‘– )2 + Î»
2ğ‘š à·
ğ‘—=1
ğ‘›
ğ‘¤ğ‘—
2
Page
Page number
2
of 42
Machine learning
system design
K-fold cross-
validation
Model selection and hyperparameter
tuning
K-Fold Cross-Validation (Model
Selection)Figure from Sebastian
Raschka
K-Fold Cross-Validation (Model
Selection)
K-Fold Cross-Validation (Model
Selection)
Machine learning
system design
Error analysis
